# MyApacheAirflow

This is my sample apache airflow dags that manage and run automatic python and bash script . 

Here the flow process of ehealth utilization dags (sqmlogic_ehealth_utilization.py)
![flow_process](https://user-images.githubusercontent.com/25681997/97281397-0be6a200-1870-11eb-862c-88ea29d12c5b.jpg)

1. The first task is executed (merge_file_ehealth.sh), it collect small csv logs (generated by antoher system)
2. Merge_file_ehealth.sh merge data into 1 file csv. 
3. The second task is executed (cleansing_utilization.py), it is python script and using pandas package .
4. The second task read data, cleansing and transform the data. 
5. The result of second task store as new file called clean CSV
6. The last task (move-to-staging.sh) will move clean csv into staging zone hadoop .

All those process running automatically and scheduled every 10 minutes and monitored by telegram bot.

Below is the dags graph view of sqmlogic_ehealth_utilization

![photo_2020-10-27_16-43-12](https://user-images.githubusercontent.com/25681997/97284668-eb204b80-1873-11eb-8b8f-f32fc1e0cc2e.jpg)

